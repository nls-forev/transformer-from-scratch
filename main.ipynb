{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Transformer Encoder from Scratch (Step-by-Step)\n",
    "\n",
    "This guide will walk you through building the Transformer Encoder from scratch using PyTorch, based on the paper [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "> We'll cover everything step-by-step with code, explanations, and intuition. You don't need to have read the full paper beforehand.\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Do Here\n",
    "\n",
    "-   Understand what self-attention is and why it's useful.\n",
    "-   Learn how each component (like multi-head attention, feed-forward layers) fits into the encoder.\n",
    "-   Implement every part from scratch, with explanations.\n",
    "-   Come away with a working Transformer encoder model that you understand.\n",
    "\n",
    "---\n",
    "\n",
    "## GitHub Repository\n",
    "\n",
    "The full code, architecture diagram, and README are available in the GitHub repo:\n",
    "[github.com/nls-forev/transformer-from-scratch](https://github.com/nls-forev/transformer-from-scratch)\n",
    "\n",
    "---\n",
    "\n",
    "> You can run each cell and follow along. If you get stuck, reread the explanations or experiment with the code. I assume you have a basic understanding of tensor operations and some sequential data processing concepts.\n",
    "\n",
    "Let’s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the architecture we will implement\n",
    "\n",
    "![encoder_architecture](encoder_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The goal is to teach our model to understand words and generate meaningful sentences.\n",
    "\n",
    "Consider the sentence:\n",
    "\n",
    "> **\"I'm a programmer. I build stuff.\"**\n",
    "\n",
    "The model needs to determine what you're talking about and how each word relates to the others. This involves understanding grammar, word order, and meaning simultaneously.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What came before Transformers?\n",
    "\n",
    "Before Transformers, Recurrent Neural Networks (RNNs) were common.\n",
    "\n",
    "In an RNN, the model processes words sequentially:\n",
    "- First, it sees `\"Im\"`, processes it, and stores information in its memory.\n",
    "- Then, it sees `\"a\"`, processes it, and updates its memory.\n",
    "- This continues for `\"programmer\"`, and so on.\n",
    "\n",
    "This approach worked reasonably well for simpler tasks. However, RNNs tend to struggle with longer sentences or more complex grammar.\n",
    "\n",
    "![rnn](rnn.png)\n",
    "\n",
    "In the image above:\n",
    "\n",
    "-   `h₀` represents the initial memory state; the model starts with no prior information.\n",
    "-   `x₁` is the first word, `\"Im\"`.\n",
    "-   The RNN processes `x₁`, updates its memory (hidden state `h`), and produces an output `y₁`.\n",
    "\n",
    "This process repeats:\n",
    "\n",
    "-   `x₂` is `\"a\"`, `x₃` is `\"programmer\"`, and so forth.\n",
    "-   Each word updates the memory (`h`) and yields a new output (`y`).\n",
    "-   `'n'` denotes the number of words in the sentence, so this repeats for `n` time steps.\n",
    "\n",
    "When the last word is processed, the memory is updated a final time, and the model produces `yₙ`, the output for the last word.\n",
    "\n",
    "---\n",
    "\n",
    "So, what is this `yₙ` output?\n",
    "\n",
    "It depends on the task the model is trained for:\n",
    "\n",
    "-   For **translation**, `yₙ` could be part of the translated sentence.\n",
    "-   For **text classification**, `yₙ` might represent the model’s decision (e.g., positive or negative sentiment).\n",
    "-   For a **language model**, it might predict the next word.\n",
    "\n",
    "In essence, `yₙ` is the final output used to solve the task, based on all information processed up to the last word.\n",
    "\n",
    "But there's a problem...\n",
    "\n",
    "As more words are processed, the memory is continually overwritten. Information from the beginning of the sentence can diminish or be lost.\n",
    "\n",
    "This is known as the **vanishing memory problem**, a significant limitation of RNNs.\n",
    "\n",
    "Let’s see how Transformers address this.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Transformers? (The Intuition)\n",
    "\n",
    "Humans can read multiple words simultaneously and grasp the overall meaning of a sentence. We don't process word-by-word like an RNN trying to memorize everything; our brains intuitively identify and focus on important words.\n",
    "\n",
    "Consider this example:\n",
    "\n",
    "> **\"That person is a thief, he was caught robbing a store.\"**\n",
    "\n",
    "Here:\n",
    "-   `\"That\"` refers to **the thief**.\n",
    "-   `\"He\"` also refers to **the thief**.\n",
    "\n",
    "Your brain makes these connections almost instantly.\n",
    "\n",
    "RNNs, however, process one word at a time, without a full understanding of how earlier or later words interrelate. When an RNN encounters `\"That\"` and later `\"He\"`, it might miss the connection, especially in long or complex sentences.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How RNNs Process Sentences (One Word at a Time)\n",
    "\n",
    "Let’s take a sentence:\n",
    "\n",
    "```python\n",
    "sentence_words = [\"Im\", \"a\", \"programmer\"]\n",
    "# rnn = RNN() # Conceptual RNN\n",
    "# hidden_states_over_time = []\n",
    "# for word in sentence_words:\n",
    "#     output, hidden_state = rnn(word, previous_hidden_state)\n",
    "#     hidden_states_over_time.append(hidden_state)\n",
    "\n",
    "# print(hidden_states_over_time[-1]) # This would be the RNN’s final memory state.\n",
    "```\n",
    "\n",
    "The hidden state (or matrix) stores what the model has \"remembered\" so far, updating after each word. The RNN reads the sentence sequentially, updating its memory and trying to retain important information by the end.\n",
    "\n",
    "## But Wait... There’s a Simpler Way to Think About This\n",
    "\n",
    "RNNs process sequentially and try to remember everything. What if we approached it differently?\n",
    "\n",
    "What if we could:\n",
    "\n",
    "-   Process **all words at once**, instead of one by one.\n",
    "-   **Pay more attention** to the important words in the sentence.\n",
    "\n",
    "Let’s give these ideas names:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parallelization\n",
    "\n",
    "**Parallel** means \"all at once\".\n",
    "\n",
    "Instead of reading words sequentially (like RNNs), Transformers feed the **entire sentence** into the model in one go. They process all words simultaneously. This makes them faster and contributes to their power.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Attention\n",
    "\n",
    "We don't want to treat all words equally.\n",
    "\n",
    "Just as your brain focuses on `\"thief\"`, `\"robbing\"`, and `\"store\"` in the sentence:\n",
    "\n",
    "> **\"That person is a thief. He was caught robbing a store.\"**\n",
    "\n",
    "Transformers have a built-in mechanism that allows them to **pay more attention to the words that actually matter** for understanding the context of any given word.\n",
    "\n",
    "---\n",
    "\n",
    "Together, **Parallelization** and **Attention** are key to the effectiveness of Transformers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![attention_and_parallelization](attention_parallel.png)\n",
    "\n",
    "In the image above:\n",
    "\n",
    "-   Notice how words are interconnected—this illustrates Attention.\n",
    "-   In RNNs, words (`x₁`, `x₂`, `x₃`, etc.) are connected only through the sequential memory, not directly to each other.\n",
    "-   With Attention, each word can \"look at\" every other word. It gives the model a comprehensive view of the entire sentence.\n",
    "\n",
    "The word `\"thief\"` (conceptually highlighted) shows the Attention mechanism focusing more on it. Notice how `\"That\"` and `\"He\"` both relate to `\"thief\"`; Attention helps the model establish these connections.\n",
    "\n",
    "---\n",
    "\n",
    "Let’s now break down how Attention works.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But Wait — How Do We Feed Words Into a Model?\n",
    "\n",
    "Before diving into Attention, we need to address a fundamental issue:\n",
    "\n",
    "> Models understand numbers, not text.\n",
    "\n",
    "If we give the model:\n",
    "\n",
    "```python\n",
    "sentence_text = [\"That\", \"person\", \"is\", \"a\", \"thief\"]\n",
    "```\n",
    "\n",
    "It doesn't inherently know what \"*thief*\" or \"*person*\" means. We need to convert each word into a **vector** of numbers. This is called a **word embedding**. A vector is just an array of numbers.\n",
    "\n",
    "## What Is a Word Embedding?\n",
    "\n",
    "We assign each word a **vector**—a fixed-size array of numbers—that captures its meaning.\n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "\"thief\"  → [0.25, -0.6, 0.1, ...]\n",
    "\"person\" → [0.88, 0.14, -0.34, ...]\n",
    "```\n",
    "\n",
    "These vectors are learned during model training, so words with similar meanings will have similar embeddings.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Do We Do That in Code?\n",
    "\n",
    "In PyTorch, we use `nn.Embedding`.\n",
    "\n",
    "Here's the basic idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Suppose we have a vocabulary of 10,000 unique words.\n",
    "vocabulary_size = 10000\n",
    "\n",
    "# We map each word to an integer index.\n",
    "# Example:\n",
    "# Dictionary = {\n",
    "#     \"that\": 0,\n",
    "#     \"person\": 1,\n",
    "#     \"is\": 2,\n",
    "#     \"a\": 3,\n",
    "#     \"thief\": 4,\n",
    "#     # ... and so on ...\n",
    "#     \".\": 9999\n",
    "# }\n",
    "\n",
    "# Each word will be represented by a vector of, say, 512 numbers.\n",
    "embedding_dimension = 512\n",
    "\n",
    "# This layer will learn an embedding vector for each word in the vocabulary.\n",
    "embedding_layer = nn.Embedding(vocabulary_size, embedding_dimension)\n",
    "\n",
    "# Let's get the embedding for the word at index 4 (e.g., \"thief\").\n",
    "word_index = torch.tensor([4]) # Input must be a tensor of indices.\n",
    "\n",
    "word_vector = embedding_layer(word_index)\n",
    "\n",
    "print(word_vector.shape)  # Output: torch.Size([1, 512])\n",
    "# This means we have 1 vector (for the one word index we passed) of size 512."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's talk about positional information.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding — Giving the Model a Sense of Word Order\n",
    "\n",
    "We've converted words into vectors (embeddings). But there's a problem:\n",
    "\n",
    "> How does the model know the position of each word in the sentence if we process all words at once?\n",
    "\n",
    "Consider:\n",
    "\n",
    "-   **\"That person is a thief\"**\n",
    "-   **\"A thief is that person\"**\n",
    "\n",
    "These sentences use the same words but have different meanings due to word order.\n",
    "\n",
    "Embeddings alone don't capture position. To an embedding layer, `\"thief\"` is `[0.1, 0.4, -0.2, ...]` regardless of its location in the sentence.\n",
    "\n",
    "We need to inform the model:\n",
    "*\"This word is first, this one is second, and that one is last.\"*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Solution: Positional Encoding\n",
    "\n",
    "We **add another vector** to each word's embedding that encodes its position.\n",
    "\n",
    "So, instead of just:\n",
    "`word_embedding = [0.4, -0.2, 0.6, ...]`\n",
    "\n",
    "The input to the model for a word becomes:\n",
    "`final_word_representation = word_embedding + positional_embedding`\n",
    "\n",
    "## What Does Positional Encoding Look Like?\n",
    "\n",
    "There are a few methods:\n",
    "\n",
    "-   The original Transformer paper used **sinusoidal functions** (sine and cosine waves of different frequencies).\n",
    "-   A simpler and often effective method is to use a **learnable embedding layer** for positions, similar to word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's See an Example\n",
    "\n",
    "Imagine the sentence: **\"Hello there!\"**\n",
    "\n",
    "First, we embed the words:\n",
    "```\n",
    "\"Hello\" → [ 0.15, -0.23, 0.99, ... ] (word embedding)\n",
    "\"there\" → [-0.37, 0.10, 0.52, ... ] (word embedding)\n",
    "\"!\"     → [-0.11, 0.64, 0.71, ... ] (word embedding)\n",
    "```\n",
    "\n",
    "Then, we generate positional embeddings (e.g., using a learnable embedding layer for positions 0, 1, 2):\n",
    "```\n",
    "Position 0 → [ 0.01, 0.03, -0.02, ... ] (positional embedding)\n",
    "Position 1 → [-0.04, 0.07, 0.05, ... ] (positional embedding)\n",
    "Position 2 → [ 0.10, -0.01, 0.00, ... ] (positional embedding)\n",
    "```\n",
    "\n",
    "Finally, we add them:\n",
    "```\n",
    "Final for \"Hello\" = [ 0.15, -0.23, 0.99, ... ] + [ 0.01, 0.03, -0.02, ... ]\n",
    "Final for \"there\" = ... (word_embedding_for_there + positional_embedding_for_pos_1)\n",
    "Final for \"!\"     = ... (word_embedding_for_! + positional_embedding_for_pos_2)\n",
    "```\n",
    "Now each word vector carries information about both its meaning and its position.\n",
    "\n",
    "Let's try an example in Python.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 0, 4, 5]])\n",
      "\"the\" -> tensor([ 0.9399, -0.4301, -1.7944,  1.5583, -1.3172,  1.9454,  1.5460,  0.1443,\n",
      "        -1.3288,  0.0594])\n",
      "\"cat\" -> tensor([ 2.0866,  0.9053,  0.5008, -0.2492,  1.1334, -1.2913,  0.4913, -1.3647,\n",
      "         0.3531,  0.8475])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "vocabulary = {\n",
    "    \"the\": 0,\n",
    "    \"cat\": 1,\n",
    "    \"sat\": 2,\n",
    "    \"on\": 3,\n",
    "    \"mat\": 4,\n",
    "    \".\": 5,\n",
    "}\n",
    "\n",
    "# Representing the sentence: \"the cat sat on the mat .\"\n",
    "# Indices:                 [  0,   1,   2,  3,   0,   4,  5]\n",
    "sentence_indices = [\n",
    "    [  # Batch dimension (1 sentence)\n",
    "        vocabulary[\"the\"],\n",
    "        vocabulary[\"cat\"],\n",
    "        vocabulary[\"sat\"],\n",
    "        vocabulary[\"on\"],\n",
    "        vocabulary[\"the\"],  # \"the\" appears again\n",
    "        vocabulary[\"mat\"],\n",
    "        vocabulary[\".\"],\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Convert to a PyTorch tensor.\n",
    "sentence_tensor = torch.tensor(\n",
    "    sentence_indices, dtype=torch.long\n",
    ")  # Use torch.long for indices\n",
    "\n",
    "print(sentence_tensor)\n",
    "\n",
    "vocabulary_size = len(vocabulary)  # Should be 6 unique words\n",
    "embedding_dimension = 10  # Our choice for the size of embedding vectors\n",
    "word_embedding_layer = nn.Embedding(\n",
    "    num_embeddings=vocabulary_size, embedding_dim=embedding_dimension\n",
    ")\n",
    "\n",
    "# Let's look at the learned embeddings (they are initialized randomly)\n",
    "# print(word_embedding_layer.weight.data)\n",
    "# We have `vocabulary_size` rows and `embedding_dimension` columns.\n",
    "print('\"the\" ->', word_embedding_layer.weight.data[vocabulary[\"the\"]])\n",
    "print('\"cat\" ->', word_embedding_layer.weight.data[vocabulary[\"cat\"]])\n",
    "\n",
    "# Convert our sentence indices to embedded vectors.\n",
    "embedded_sentence = word_embedding_layer(sentence_tensor)\n",
    "# print(embedded_sentence.shape) # torch.Size([1, 7, 10]) -> (batch_size, sequence_length, embedding_dimension)\n",
    "\n",
    "# Now, let's add positional embeddings.\n",
    "# We need positions for each word in the sentence.\n",
    "max_sequence_length = sentence_tensor.shape[1]  # Length of our sentence, 7\n",
    "position_indices = torch.arange(max_sequence_length).unsqueeze(0)  # Shape: [1, 7]\n",
    "# print(position_indices)\n",
    "\n",
    "# Assuming max_sequence_length is the maximum number of positions we need to encode.\n",
    "positional_embedding_layer = nn.Embedding(\n",
    "    num_embeddings=max_sequence_length, embedding_dim=embedding_dimension\n",
    ")\n",
    "positional_encodings = positional_embedding_layer(position_indices)\n",
    "# print(positional_encodings.shape) # torch.Size([1, 7, 10])\n",
    "\n",
    "# Add positional encodings to the word embeddings.\n",
    "final_input_sentence = embedded_sentence + positional_encodings\n",
    "# This `final_input_sentence` is what the model receives.\n",
    "# print(final_input_sentence.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Attention?\n",
    "\n",
    "So far:\n",
    "-   Words are turned into vectors (embeddings).\n",
    "-   Positional information is added (positional encoding).\n",
    "\n",
    "Now we arrive at the core of the Transformer: the Attention mechanism.\n",
    "This is where the model determines:\n",
    "> “For understanding this specific word, which other words in the sentence should I focus on?”\n",
    "\n",
    "Using our earlier example:\n",
    "> **\"That person is a thief. He was caught robbing a store.\"**\n",
    "\n",
    "When the model processes `\"He\"`, how does it know that `\"He\"` refers to `\"That person\"` or `\"thief\"`? This is where Attention helps.\n",
    "\n",
    "Attention allows the model to consider all words simultaneously and figure out:\n",
    "-   Which words are most relevant to the current word being processed.\n",
    "-   How much importance to assign to each of these relevant words.\n",
    "-   How to combine information from these words to enrich the representation of the current word.\n",
    "\n",
    "It’s like giving the model the ability to connect the dots between words dynamically.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Does It Work (In Simple Terms)?\n",
    "\n",
    "Let’s say the model is focusing on the word `\"He\"`.\n",
    "\n",
    "It asks:\n",
    "> “Which other words in this sentence are important for understanding 'He'?”\n",
    "\n",
    "-   It might assign a high relevance score to `\"thief\"`.\n",
    "-   A medium score to `\"store\"`.\n",
    "-   Low scores to `\"caught\"` or `\"is\"`.\n",
    "\n",
    "Then, it uses these scores to create a weighted combination of information from the other words.\n",
    "So, the representation of `\"He\"` gets updated with a context-aware vector, emphasizing the most relevant parts of the sentence for understanding `\"He\"`.\n",
    "\n",
    "That's the essence of Attention.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You Can Think of It Like This:\n",
    "\n",
    "Imagine a spotlight moving across the sentence for each word.\n",
    "When focusing on one word, the model can shine the spotlight more intensely on other words that are highly relevant to it, and dim the light on less relevant ones.\n",
    "\n",
    "-   High attention → brighter light → more influence.\n",
    "-   Low attention → dimmer light → less influence.\n",
    "\n",
    "Each word effectively gets to \"query\" all other words and gather contextual information.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Up: How Does the Model Actually Do This?\n",
    "\n",
    "To implement this, we associate three vectors with each word (or more accurately, with each word's embedding):\n",
    "\n",
    "-   **Query vector (Q)**: Represents the current word \"asking\" for information. \"What am I looking for?\"\n",
    "-   **Key vector (K)**: Represents other words \"offering\" information. \"What information do I have?\"\n",
    "-   **Value vector (V)**: Represents the actual content/information of these other words. \"Here is the information I hold.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Do We Actually Implement Attention?\n",
    "\n",
    "Let's build intuition for how Query, Key, and Value vectors work in practice.\n",
    "\n",
    "Imagine you're in a class and need notes on a specific topic. You don't know everything, so you ask around. This is your:\n",
    "> **Query** → *\"Can someone help me with information about X?\"*\n",
    "\n",
    "Not everyone in class will have relevant notes. Only some students have information pertaining to X. These students and their relevance are identified by their:\n",
    "> **Keys** → *\"I have notes on topic X!\"* or *\"My notes are about topic Y.\"*\n",
    "\n",
    "And the actual notes they share with you, if their Key matches your Query, are the:\n",
    "> **Values** → *\"Here are my detailed notes on X.\"*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar process happens in the model.\n",
    "Consider the sentence:\n",
    "> **\"The cat sat on the mat.\"**\n",
    "\n",
    "For each word, its Query vector will be compared against the Key vectors of all other words (including itself).\n",
    "-   `\"The\"` (as a Query) asks: *\"Which words are relevant to me?\"*\n",
    "-   `\"cat\"` (as a Query) asks the same, and so on for every word.\n",
    "\n",
    "Each word scores every other word based on how relevant its Key is to the current word's Query.\n",
    "Then, it gathers information (Value vectors) from the other words, weighted by these relevance scores.\n",
    "\n",
    "So, each word's representation gets updated with context from the entire sentence. This is self-attention.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now break this down mathematically and implement **Scaled Dot-Product Attention**.\n",
    "\n",
    "## ⚙️ Scaled Dot-Product Attention (Without the Scary Math)\n",
    "\n",
    "Alright — time to actually build the attention mechanism.\n",
    "\n",
    "Remember:\n",
    "- Every word's embedding is transformed into a **Query (Q)** — it asks: *\"Whom should I look at?\"*\n",
    "- Every word's embedding is also transformed into a **Key (K)** — it says: *\"This is what I have/represent.\"*\n",
    "- And every word's embedding is transformed into a **Value (V)** — it holds the actual info to be shared.\n",
    "\n",
    "So now, for each word, the model will:\n",
    "\n",
    "1.  **Compare its Query (Q)** with every **Key (K)** in the sentence using a dot product. This measures similarity.\n",
    "2.  This gives us **attention scores** — higher scores mean “pay more attention.”\n",
    "3.  We **scale** these scores (divide by the square root of the dimension of K) to stabilize gradients, then apply **Softmax** to turn them into weights (which sum to 1, like probabilities).\n",
    "4.  Then we **multiply these weights by the Value (V) vectors** and sum them up to get a new, contextually informed representation for the word.\n",
    "\n",
    "That’s it!\n",
    "\n",
    "Lets implement it.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 10])\n",
      "Q shape: torch.Size([1, 7, 10])\n",
      "K shape: torch.Size([1, 7, 10])\n",
      "V shape: torch.Size([1, 7, 10])\n",
      "Attention scores shape: torch.Size([1, 7, 7])\n",
      "Attention scores (raw): tensor([[[-0.0634,  1.7923, -0.8856, -0.3646, -1.8662, -0.2431, -0.0258],\n",
      "         [-1.4564, -0.8277, -1.7748, -0.2889, -0.9740, -2.1767,  0.0072],\n",
      "         [ 0.3088, -1.3835,  2.2364, -2.1196,  3.4828,  0.1128,  4.1532],\n",
      "         [-1.8114, -1.3909, -0.2429, -0.2396,  0.9032, -0.9812,  0.7299],\n",
      "         [-0.5266, -0.1555,  0.7412, -0.4427,  1.5388,  0.5065,  1.2252],\n",
      "         [ 0.7598, -0.5155,  2.4611, -2.3154,  1.9251,  1.1923,  1.2763],\n",
      "         [ 1.5280, -2.3244, -1.2958, -2.0178,  1.3236, -2.0623,  1.1563]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's use the 'final_input_sentence' from the positional encoding example.\n",
    "batch_size = 1\n",
    "sequence_length = 7\n",
    "embedding_dimension = 10 \n",
    "\n",
    "sentence = final_input_sentence\n",
    "print(sentence.shape)  # Should be [1, 7, 10]\n",
    "\n",
    "# These linear layers will project the input embeddings into Q, K, V spaces.\n",
    "\n",
    "# Create the Query, Key, and Value projection layers.\n",
    "# These are learnable weight matrices.\n",
    "query_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
    "key_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
    "value_layer = nn.Linear(embedding_dimension, embedding_dimension, bias=False)\n",
    "\n",
    "# Project sentence embeddings to Q, K, V\n",
    "Q = query_layer(sentence)  # Shape: (batch_size, sequence_length, embedding_dimension)\n",
    "K = key_layer(sentence)  # Shape: (batch_size, sequence_length, embedding_dimension)\n",
    "V = value_layer(\n",
    "    sentence\n",
    ")  # Shape: (batch_size, sequence_length, embedding_dimension) # or embedding_dimension\n",
    "\n",
    "print(\"Q shape:\", Q.shape)\n",
    "print(\"K shape:\", K.shape)\n",
    "print(\"V shape:\", V.shape)\n",
    "\n",
    "# Compute attention scores: Q * K_transpose\n",
    "# K.transpose(-2, -1) transposes the last two dimensions (sequence_length, embedding_dimension) -> (embedding_dimension, sequence_length)\n",
    "attention_scores = Q @ K.transpose(-2, -1)\n",
    "# Resulting shape: (batch_size, sequence_length, sequence_length)\n",
    "# This matrix tells how much each word (row) should attend to every other word (column).\n",
    "\n",
    "\n",
    "print(\"Attention scores shape:\", attention_scores.shape)\n",
    "print(\"Attention scores (raw):\", attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how every word attends to every other word in the sentence.\n",
    "\n",
    "![self attention map](attention_map.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights shape: torch.Size([1, 7, 7])\n",
      "Attention weights (after softmax): tensor([[[0.0942, 0.6026, 0.0414, 0.0697, 0.0155, 0.0787, 0.0978],\n",
      "         [0.0755, 0.1416, 0.0549, 0.2427, 0.1223, 0.0367, 0.3263],\n",
      "         [0.0126, 0.0023, 0.0863, 0.0011, 0.3003, 0.0103, 0.5870],\n",
      "         [0.0237, 0.0361, 0.1137, 0.1140, 0.3576, 0.0543, 0.3007],\n",
      "         [0.0425, 0.0615, 0.1509, 0.0462, 0.3349, 0.1193, 0.2448],\n",
      "         [0.0756, 0.0211, 0.4143, 0.0035, 0.2424, 0.1165, 0.1267],\n",
      "         [0.3785, 0.0080, 0.0225, 0.0109, 0.3086, 0.0104, 0.2610]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Sum of weights for first word: tensor(1., grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Once we compute Q @ K, we apply an activation called Softmax, Softmax will signal the word whom is paying how much attention, whom is paying the most, the least, etc.\n",
    "# dim=-1 means softmax is applied along the last dimension (each row sums to 1)\n",
    "attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "print(\"Attention weights shape:\", attention_weights.shape)\n",
    "print(\"Attention weights (after softmax):\", attention_weights)\n",
    "print(\n",
    "    \"Sum of weights for first word:\", attention_weights[0, 0, :].sum()\n",
    ")  # Should be close to 1\n",
    "\n",
    "# Visualize attention weights for the first (and only) sentence in the batch\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(\n",
    "    attention_weights.squeeze(0).detach().cpu().numpy(),\n",
    "    cmap=\"viridis\",\n",
    "    interpolation=\"nearest\",\n",
    ")\n",
    "plt.title(\"Attention Weights\")\n",
    "plt.xlabel(\"Key Positions (Words being attended to)\")\n",
    "plt.ylabel(\"Query Positions (Words doing the attending)\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# The more the yellow it is, the more it paid attention. First cell is the word \"the\" and \"the\" itself.\n",
    "# The second cell is the word \"the\" and \"cat, and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 7, 10])\n",
      "Output of attention: tensor([[[ 0.3987, -0.2925,  0.0132,  0.5659, -0.2349, -0.2188,  0.3974,\n",
      "           0.2512, -0.0958, -0.4393],\n",
      "         [ 0.7310, -0.2600, -0.0018,  0.0701, -0.3640, -0.1686,  0.5299,\n",
      "           0.2414, -0.0220, -0.0991],\n",
      "         [ 1.4112, -0.3895, -0.0199, -0.4825, -0.6530, -0.0406,  0.9999,\n",
      "           0.5630, -0.1254,  0.0761],\n",
      "         [ 0.7180, -0.2356, -0.1699, -0.2698, -0.4807,  0.0569,  0.4236,\n",
      "           0.5610, -0.1970, -0.5101],\n",
      "         [ 0.6738, -0.1633, -0.2804, -0.2992, -0.3718,  0.1905,  0.3844,\n",
      "           0.6223, -0.2318, -0.6423],\n",
      "         [ 0.7267, -0.1101, -0.4679, -0.3680, -0.2611,  0.3927,  0.2890,\n",
      "           0.7074, -0.2799, -0.8708],\n",
      "         [ 0.5379,  0.0567, -0.2975, -0.5639, -0.3129,  0.0129,  0.8440,\n",
      "           0.5001, -0.2772, -0.5530]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Original sentence embedding for first word: tensor([ 0.8984,  0.2571, -0.0590,  1.6468, -1.6583], grad_fn=<SliceBackward0>)\n",
      "Attention output for first word: tensor([ 0.3987, -0.2925,  0.0132,  0.5659, -0.2349], grad_fn=<SliceBackward0>)\n",
      "Combined (residual) for first word: tensor([ 1.2971, -0.0354, -0.0459,  2.2127, -1.8932], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Now we compute the output, to get the actual value each word contains. The word will get most of the value from the word it paid the most attention to and the rest distributed to other words which aren't that important.\n",
    "output = attention_weights @ V\n",
    "# Shape: (batch_size, sequence_length, d_k) - each word's representation is now a weighted sum of V's\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Output of attention:\", output)\n",
    "\n",
    "sentence_after_attention = (\n",
    "    sentence + output\n",
    ")  # Now, why did we add? Because we want our model to still remember the actual input we gave. There's also one more reason, its called Residual which is H(x) = F(x) + y => F(x) = H(x) - y, but we are not gonna talk about it yet.\n",
    "\n",
    "print(\"Original sentence embedding for first word:\", sentence[0, 0, :5])\n",
    "print(\"Attention output for first word:\", output[0, 0, :5])\n",
    "print(\"Combined (residual) for first word:\", sentence_after_attention[0, 0, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After LayerNorm for first word: tensor([ 0.7686, -0.1809, -0.1884,  1.4211, -1.5049], grad_fn=<SliceBackward0>)\n",
      "Mean of first word features after norm: tensor(0., grad_fn=<MeanBackward0>)\n",
      "Std of first word features after norm: tensor(1.0541, grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Layer Normalization\n",
    "# We add a layer norm, which is basically standardizing the entire matrix row by row.\n",
    "# Why? because we wanna make sure training is stabilized and gradients dont explode.\n",
    "# LayerNorm normalizes across the features (embedding_dimension) for each item in the batch and sequence.\n",
    "\n",
    "layer_norm1 = nn.LayerNorm(normalized_shape=embedding_dimension)\n",
    "sentence_after_norm = layer_norm1(sentence_after_attention)\n",
    "\n",
    "print(\"After LayerNorm for first word:\", sentence_after_norm[0, 0, :5])\n",
    "print(\n",
    "    \"Mean of first word features after norm:\", sentence_after_norm[0, 0, :].mean()\n",
    ")  # Should be close to 0\n",
    "print(\n",
    "    \"Std of first word features after norm:\", sentence_after_norm[0, 0, :].std()\n",
    ")  # Should be close to 1\n",
    "\n",
    "# Add & Norm step 1 (after attention)\n",
    "x = sentence + output  # Residual connection\n",
    "x = layer_norm1(x)  # Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, 'x' is the output of the self-attention sub-layer.\n",
    "# Next is the Feed-Forward Network (FFN) sub-layer.\n",
    "\n",
    "# The Feed-Forward Network usually consists of two linear layers with a ReLU activation in between.\n",
    "# The first linear layer expands the dimension, and the second compresses it back.\n",
    "ffn_hidden_dim = 4 * embedding_dimension # Commonly 4 times the embedding dimension\n",
    "feed_forward_network = nn.Sequential(\n",
    "    nn.Linear(embedding_dimension, ffn_hidden_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(ffn_hidden_dim, embedding_dimension),\n",
    ")\n",
    "\n",
    "# Pass the output of the attention sub-layer through the FFN\n",
    "ffn_output = feed_forward_network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need a feed forward network?\n",
    "\n",
    "> Well what does a neural network do? learn patterns in data right? we do the same here, we want to learn the patterns in our sentence. We want to learn the grammar, the context, etc. For example it could learn that \"cat\" is a noun and \"sat\" is a verb, or that \"mat\" is a noun too. All we did before was compute attention between each word but we specifically didn't make the model learn the patterns right? Well we do that step now! For example the word \"bank\" has several meanings depending on the context. River bank and State bank have totally different meanings right? All we did was compute attention between these words but we never really learnt what they actually meant depending on the context. This is why need a seperate neural network to understand the difference between words too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output of encoder block for first word: tensor([ 0.6956, -0.1996, -0.3969,  1.4062, -1.3446], grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 7, 10])\n"
     ]
    }
   ],
   "source": [
    "# Add & Norm step 2 (after FFN)\n",
    "# Create another LayerNorm for the FFN output\n",
    "layer_norm2 = nn.LayerNorm(normalized_shape=embedding_dimension)\n",
    "encoder_block_output = x + ffn_output # Residual connection\n",
    "encoder_block_output = layer_norm2(encoder_block_output) # Layer Normalization\n",
    "\n",
    "# 'encoder_block_output' is the final output of one Transformer Encoder block.\n",
    "print(\"Final output of encoder block for first word:\", encoder_block_output[0,0,:5])\n",
    "print(encoder_block_output.shape) # Should be [1, 7, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that completes one pass through a Transformer Encoder block! We've covered word embeddings, positional encoding, scaled dot-product attention, residual connections, layer normalization, and the feed-forward network.\n",
    "\n",
    "This `encoder_block_output` now contains richer, context-aware representations of each word in the input sentence. In a full Transformer model, you might stack several of these encoder blocks.\n",
    "\n",
    "This gives a foundational understanding of the components. The next step would be to encapsulate these into reusable modules (like a `MultiHeadAttention` module, an `EncoderBlock` class) to build a full Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with the building block for attention: a single attention head.\n",
    "# This class is all about creating those Q, K, and V matrices we talked about from the input.\n",
    "class SingleAttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A single attention head.\n",
    "    It takes the input embeddings and projects them into Query, Key, and Value spaces.\n",
    "    Think of it as one perspective or 'head' looking at the input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, head_dim: int):\n",
    "        # embed_dim: This is the dimension of the input token embeddings (e.g., 512).\n",
    "        # head_dim: This is the dimension for the Q, K, V for this specific head (e.g., 64 if embed_dim is 512 and you have 8 heads).\n",
    "        super().__init__()\n",
    "        # These are just linear layers (learnable weight matrices) to transform\n",
    "        # the input 'x' into Q, K, and V. No bias needed, as per the paper.\n",
    "        self.w_q = nn.Linear(embed_dim, head_dim, bias=False) # Query projection\n",
    "        self.w_k = nn.Linear(embed_dim, head_dim, bias=False) # Key projection\n",
    "        self.w_v = nn.Linear(embed_dim, head_dim, bias=False) # Value projection\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass for SingleAttentionHead.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "                              Shape: [batch_size, seq_len, embed_dim].\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing Q, K, and V tensors.\n",
    "                                                             Each tensor shape: [batch_size, seq_len, head_dim].\n",
    "        \"\"\"\n",
    "        # x shape: [batch_size, seq_len, embed_dim]\n",
    "        # We just pass the input 'x' through our linear layers.\n",
    "        Q = self.w_q(x) # Shape: [batch_size, seq_len, head_dim]\n",
    "        K = self.w_k(x) # Shape: [batch_size, seq_len, head_dim]\n",
    "        V = self.w_v(x) # Shape: [batch_size, seq_len, head_dim]\n",
    "        return Q, K, V\n",
    "\n",
    "# Now, why stop at one head? Multi-Head Attention is where the magic really happens.\n",
    "# We're basically doing what we did in SingleAttentionHead, but multiple times in parallel.\n",
    "# Each head can learn different aspects of the relationships between words.\n",
    "# One head might focus on syntactic relationships, another on semantic ones,\n",
    "# one might track short-range dependencies, another long-range. It's like having a committee of experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention module.\n",
    "    It creates multiple SingleAttentionHead instances and concatenates their Q, K, V outputs.\n",
    "    Note: This module *prepares* Q, K, V for multi-head attention; the actual attention\n",
    "    calculation (scaled dot-product) happens outside this specific class in the TransformerBlock.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, heads: int, embed_dim: int):\n",
    "        # heads: The number of parallel attention heads we want (e.g., 8).\n",
    "        # embed_dim: Total dimension of the input token embeddings (e.g., 512).\n",
    "        super().__init__()\n",
    "        # It's crucial that the embedding dimension is divisible by the number of heads.\n",
    "        # This way, we can split the 'embed_dim' evenly among the heads.\n",
    "        assert embed_dim % heads == 0, \"Embedding dimension must be divisible by the number of heads.\"\n",
    "\n",
    "        self.head_dim = embed_dim // heads # Dimension for Q, K, V for each individual head.\n",
    "        self.heads = heads                # Number of heads.\n",
    "\n",
    "        # We create a list of 'SingleAttentionHead' modules, one for each head.\n",
    "        # nn.ModuleList is important here so PyTorch recognizes these as submodules.\n",
    "        self.heads_list = nn.ModuleList(\n",
    "            [\n",
    "                SingleAttentionHead(embed_dim=embed_dim, head_dim=self.head_dim)\n",
    "                for _ in range(self.heads)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass for MultiHeadAttention.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "                              Shape: [batch_size, seq_len, embed_dim].\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing the final Q, K, and V tensors\n",
    "                                                             concatenated across all heads.\n",
    "                                                             Q_final, K_final, V_final shapes: [batch_size, seq_len, embed_dim (which is heads * head_dim)].\n",
    "        \"\"\"\n",
    "        q_list, k_list, v_list = [], [], []\n",
    "        # Get Q, K, V from each head and store them.\n",
    "        for head_module in self.heads_list:\n",
    "            # Each 'head_module' is a SingleAttentionHead instance.\n",
    "            q_single_head, k_single_head, v_single_head = head_module(x)\n",
    "            q_list.append(q_single_head)\n",
    "            k_list.append(k_single_head)\n",
    "            v_list.append(v_single_head)\n",
    "\n",
    "        # Now, concatenate the outputs from all heads along the last dimension (the feature dimension).\n",
    "        # We do this to combine what every head has learned (or rather, their Q, K, V projections).\n",
    "        # Each q_single_head, k_single_head, v_single_head has shape [batch_size, seq_len, head_dim].\n",
    "        # After cat, Q, K, V will have shape [batch_size, seq_len, heads * head_dim],\n",
    "        # which is effectively [batch_size, seq_len, embed_dim] because embed_dim = heads * head_dim.\n",
    "        Q_final = torch.cat(q_list, dim=-1)\n",
    "        K_final = torch.cat(k_list, dim=-1)\n",
    "        V_final = torch.cat(v_list, dim=-1)\n",
    "\n",
    "        return Q_final, K_final, V_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our simple feed-forward network. After attention, each token's representation\n",
    "# is passed through this network independently.\n",
    "# It's usually two linear layers with a non-linearity (like ReLU) in between.\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple position-wise feed-forward network.\n",
    "    It consists of two linear transformations with a ReLU activation in between.\n",
    "    The input is expanded to `hidden_dim` and then contracted back to `embed_dim`.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, hidden_dim: int):\n",
    "        # embed_dim: Dimension of the input (and output) features.\n",
    "        # hidden_dim: Dimension of the inner layer (usually larger, e.g., 4 * embed_dim).\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim), # Expand\n",
    "            nn.ReLU(),                       # Non-linearity\n",
    "            nn.Linear(hidden_dim, embed_dim), # Contract\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape: [batch_size, seq_len, embed_dim]\n",
    "        # Output shape: [batch_size, seq_len, embed_dim]\n",
    "        return self.net(x)\n",
    "\n",
    "# For positional information, since we process all words at once,\n",
    "# we need to explicitly tell the model about word order.\n",
    "# This class creates learnable positional embeddings.\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Learnable Positional Embeddings.\n",
    "    Creates an embedding vector for each position up to `max_seq_len`.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_seq_len: int, embed_dim: int):\n",
    "        # max_seq_len: The maximum length of an input sequence.\n",
    "        # embed_dim: The dimension of the embeddings (must match word embeddings).\n",
    "        super().__init__()\n",
    "        # We use nn.Embedding to create learnable vectors for each position.\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x is the input tensor (e.g., word embeddings), we only need its sequence length and device.\n",
    "        # x shape: [batch_size, seq_len, embed_dim]\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # Create a tensor of position indices: [0, 1, 2, ..., seq_len-1]\n",
    "        # It's important to put these indices on the same device as 'x'.\n",
    "        positions = torch.arange(seq_len, device=x.device)\n",
    "        # positions shape: [seq_len]\n",
    "\n",
    "        # Get the positional embeddings for these positions.\n",
    "        # Output shape: [seq_len, embed_dim]\n",
    "        # This will be broadcastable to add to x (if x is [batch_size, seq_len, embed_dim])\n",
    "        # or you might need to unsqueeze if adding to something already having batch.\n",
    "        # For adding to word embeddings (batch_size, seq_len, embed_dim), this works due to broadcasting.\n",
    "        return self.pos_embed(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Now, let's put it all together into one Transformer Encoder Block!\n",
    "# This block will contain:\n",
    "# 1. Multi-Head Self-Attention\n",
    "# 2. Add & Norm (Residual connection + Layer Normalization)\n",
    "# 3. Feed-Forward Network\n",
    "# 4. Add & Norm (Again!)\n",
    "# We're following the Pre-LayerNorm structure here, which means LayerNorm is applied *before*\n",
    "# the main operation in each sub-layer. It often leads to more stable training.\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer Encoder Block.\n",
    "    Implements Multi-Head Attention and a Feed-Forward network,\n",
    "    each followed by a residual connection and layer normalization (Pre-LN style).\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        heads: int,      # Number of attention heads\n",
    "        embed_dim: int,  # Dimension of token embeddings (and model's hidden size)\n",
    "        ff_hidden_dim: int, # Hidden dimension for the FeedForward network\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = heads # Storing for reshaping logic later\n",
    "        self.head_dim = embed_dim // heads # Dimension per head\n",
    "\n",
    "        # First sub-layer: Multi-Head Attention\n",
    "        self.mha = MultiHeadAttention(heads=heads, embed_dim=embed_dim)\n",
    "        # LayerNorm for the MHA sub-layer's input\n",
    "        self.layer_norm1 = nn.LayerNorm(normalized_shape=embed_dim)\n",
    "\n",
    "        # Second sub-layer: Feed-Forward Network\n",
    "        self.ffn = FeedForward(embed_dim=embed_dim, hidden_dim=ff_hidden_dim)\n",
    "        # LayerNorm for the FFN sub-layer's input\n",
    "        self.layer_norm2 = nn.LayerNorm(normalized_shape=embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x is the input to the block, shape: [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        # --- Multi-Head Attention Sub-layer (Pre-LN) ---\n",
    "        # 1. Normalize the input 'x'. This is the \"Pre\" in Pre-LayerNorm.\n",
    "        x_norm1 = self.layer_norm1(x)\n",
    "\n",
    "        # 2. Get the Q, K, V projections, already concatenated from all heads by our MHA class.\n",
    "        # Q_cat, K_cat, V_cat shapes: [batch_size, seq_len, embed_dim]\n",
    "        Q_cat, K_cat, V_cat = self.mha(x_norm1)\n",
    "\n",
    "        batch_size, seq_len, embed_dim_total = Q_cat.shape\n",
    "        # num_heads = self.num_heads (or self.mha.heads)\n",
    "        # head_dim = self.head_dim (or self.mha.head_dim)\n",
    "\n",
    "        # 3. Reshape Q_cat, K_cat, V_cat to be per head for the actual attention calculation.\n",
    "        # We need to get from [batch_size, seq_len, embed_dim_total]\n",
    "        # to [batch_size, num_heads, seq_len, head_dim].\n",
    "        # embed_dim_total is num_heads * head_dim.\n",
    "        Q_reshaped = Q_cat.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K_reshaped = K_cat.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V_reshaped = V_cat.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # After transpose, shapes are [batch_size, num_heads, seq_len, head_dim]\n",
    "\n",
    "        # 4. Scaled Dot-Product Attention - this is the core calculation, done per head.\n",
    "        # (Q @ K.transpose) gives scores: [batch_size, num_heads, seq_len, seq_len]\n",
    "        # I didn't mention scaling before, but it's important!\n",
    "        # We scale by sqrt(head_dim) to prevent dot products from becoming too large,\n",
    "        # which helps keep gradients stable, especially when head_dim is large.\n",
    "        attention_scores = (Q_reshaped @ K_reshaped.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # Apply softmax to get attention weights (probabilities).\n",
    "        # dim=-1 ensures weights sum to 1 across each row of the seq_len x seq_len matrix.\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        # attention_weights shape: [batch_size, num_heads, seq_len, seq_len]\n",
    "\n",
    "        # Multiply weights by V_reshaped to get the weighted sum of values.\n",
    "        # (attention_weights @ V_reshaped) -> [batch_size, num_heads, seq_len, head_dim]\n",
    "        attention_output_heads = attention_weights @ V_reshaped\n",
    "\n",
    "        # 5. Concatenate heads back to the original embedding dimension.\n",
    "        # First, transpose to bring seq_len and num_heads dimensions adjacent for view:\n",
    "        # [batch_size, seq_len, num_heads, head_dim]\n",
    "        attention_output_concat = attention_output_heads.transpose(1, 2).contiguous()\n",
    "        # Then, view to merge the last two dimensions (num_heads * head_dim = embed_dim_total)\n",
    "        # Result shape: [batch_size, seq_len, embed_dim_total]\n",
    "        attention_output_final = attention_output_concat.view(batch_size, seq_len, embed_dim_total)\n",
    "        # This attention_output_final is the complete output of the MHA mechanism.\n",
    "\n",
    "        # 6. Add residual connection for the attention sub-layer.\n",
    "        # We add the output of MHA to the original input 'x' of this sub-layer (which is the block's input 'x').\n",
    "        # This helps with vanishing gradients and allows the model to learn identity functions if needed.\n",
    "        x_after_mha = x + attention_output_final\n",
    "        # --- End of Multi-Head Attention Sub-layer ---\n",
    "\n",
    "        # --- Feed-Forward Network Sub-layer (Pre-LN) ---\n",
    "        # 7. Normalize the output from the MHA sub-layer (x_after_mha).\n",
    "        x_norm2 = self.layer_norm2(x_after_mha)\n",
    "\n",
    "        # 8. Pass it through the Feed-Forward Network.\n",
    "        ffn_output = self.ffn(x_norm2)\n",
    "\n",
    "        # 9. Add residual connection for the FFN sub-layer.\n",
    "        # We add the output of FFN to the input of this FFN sub-layer (which was x_after_mha).\n",
    "        x_after_ffn = x_after_mha + ffn_output\n",
    "        # --- End of Feed-Forward Network Sub-layer ---\n",
    "\n",
    "        return x_after_ffn\n",
    "\n",
    "# And that's one Transformer Encoder block!\n",
    "# You'd typically stack several of these blocks to build the full encoder.\n",
    "# The output of one block becomes the input to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we build the full Encoder.\n",
    "# The Encoder is basically a stack of our TransformerBlocks.\n",
    "# It takes raw token IDs as input, embeds them, adds positional info,\n",
    "# and then passes them through all the blocks.\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The Transformer Encoder module.\n",
    "    It comprises an embedding layer for tokens, positional embeddings,\n",
    "    a stack of Transformer Blocks, and a final layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers_count: int,    # How many TransformerBlocks to stack (e.g., 6 or 12)\n",
    "        heads_count: int,     # Number of attention heads in each TransformerBlock\n",
    "        max_seq_len: int,   # Maximum sequence length this encoder can handle (for positional embeddings)\n",
    "        vocab_size: int,    # Size of our vocabulary (how many unique tokens)\n",
    "        embed_dim: int,     # Embedding dimension for tokens and throughout the model\n",
    "        ff_hidden_dim: int,   # Hidden dimension for the FeedForward networks inside blocks\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Token Embedding: Converts input token IDs (integers) into dense vectors.\n",
    "        # Each unique word/token in our vocab gets its own 'embed_dim'-sized vector.\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # 2. Positional Embedding: Adds information about the position of each token in the sequence.\n",
    "        # We use our previously defined learnable PositionalEmbedding class.\n",
    "        self.pos_embed = PositionalEmbedding(\n",
    "            max_seq_len=max_seq_len, embed_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        # 3. Transformer Blocks: A stack of 'layers_count' TransformerBlocks.\n",
    "        # nn.ModuleList is crucial here so PyTorch correctly registers all the blocks.\n",
    "        # Each block will perform self-attention and feed-forward operations.\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    heads=heads_count,\n",
    "                    embed_dim=embed_dim,\n",
    "                    ff_hidden_dim=ff_hidden_dim, # Make sure to pass the correct hidden_dim arg name\n",
    "                )\n",
    "                for _ in range(layers_count) # Create 'layers_count' identical blocks\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 4. Final Layer Normalization: An additional LayerNorm after all blocks have processed the input.\n",
    "        # This can help stabilize the final output of the encoder.\n",
    "        self.final_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x starts as a tensor of token IDs.\n",
    "        # Shape: [batch_size, seq_len]\n",
    "\n",
    "        # Step 1: Get token embeddings.\n",
    "        # Output shape: [batch_size, seq_len, embed_dim]\n",
    "        token_embeddings = self.token_embed(x)\n",
    "\n",
    "        # Step 2: Get positional embeddings and add them to token embeddings.\n",
    "        # self.pos_embed(token_embeddings) will generate positional embeddings for the given seq_len.\n",
    "        # The output of self.pos_embed will be [seq_len, embed_dim],\n",
    "        # which will be broadcasted across the batch dimension when added to token_embeddings.\n",
    "        # This is the final input representation before it goes into the blocks.\n",
    "        # Shape: [batch_size, seq_len, embed_dim]\n",
    "        x_with_pos = token_embeddings + self.pos_embed(token_embeddings) # Pass the embedded tensor to get seq_len and device\n",
    "\n",
    "        # Step 3: Pass the embeddings through each TransformerBlock in sequence.\n",
    "        # The output of one block becomes the input to the next.\n",
    "        processed_x = x_with_pos\n",
    "        for block in self.blocks:\n",
    "            processed_x = block(processed_x)\n",
    "        # Shape after blocks: [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        # Step 4: Apply the final layer normalization.\n",
    "        # Shape: [batch_size, seq_len, embed_dim]\n",
    "        final_output = self.final_norm(processed_x)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So we have successfully built the encoder. Well, thats it for now.\n",
    "\n",
    "#### In the next notebook we will learn how to use the encoder to classify imdb reviews for sentiment analysis.\n",
    "\n",
    "#### And soon learn how to build the decoder to create GPT style models too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
